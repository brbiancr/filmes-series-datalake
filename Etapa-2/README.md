# Etapa 2 - Camada TRUSTED

> √â a **segunda camada do Data Lake.** Nesta etapa os dados brutos da camada RAW s√£o **limpos, integrados e padronizados** usando **Apache Spark** rodando no **AWS Glue**. 
O resultado s√£o arquivos **PARQUET** otimizados, **particionados por data de ingest√£o (ano/m√™s/dia)**, armazenados em **Amazon S3**, **catalogados no Glue Data Catalog** e **consult√°veis via AWS Athena**.
> 

## üîπ **Objetivo**

- Unificar e padronizar esquemas vindos de diferentes fontes (CSV, JSON TMDB).
- Tratar e normalizar tipos de dados (datas, num√©ricos, booleanos, textos).
- Explodir arrays (como listas de g√™neros ou IDs) em linhas normalizadas.
- Remover inconsist√™ncias e dados corrompidos.
- Persistir os dados em **formato Parquet** (colunar e otimizado).
- Particionar os dados por `ano_raw`, `mes_raw`, `dia_raw` para facilitar consultas.
- Registrar as tabelas no **Glue Data Catalog** para uso no **Athena**.

![image.png](../assets/image%203.png)

## üîπ Etapa 2A ‚Äì Processamento dos CSVs

### üß© Passos do Processo

1. Leitura dos arquivos `.csv` diretamente do S3 (camada RAW).
2. Convers√£o e padroniza√ß√£o de datas e valores num√©ricos.
3. Explos√£o de colunas com m√∫ltiplos valores.
4. Escrita dos resultados no bucket `TRUSTED` em formato **Parquet**.

### üìù **Observa√ß√£o Importante**

> ‚ö†Ô∏è Os arquivos .csv originais n√£o est√£o versionados neste reposit√≥rio para evitar o armazenamento de grandes volumes de dados.
> 
> 
> Eles podem ser acessados diretamente no **bucket S3** configurado para este projeto.
> 
> Caso deseje testar localmente, h√° amostras reduzidas dos dados dispon√≠veis na pasta:
> 
> ```
> data/sample/
> ```
> 
> com apenas algumas linhas, para fins de demonstra√ß√£o.
> 

## üîπ Etapa 2B ‚Äì Processamento dos JSONs (TMDB)

### üß© Passos do Processo

1. Leitura dos arquivos `.json` do bucket `RAW/TMDB/JSON/...`.
2. Explos√£o de arrays (`results`, `genre_ids`, `genres`) em linhas individuais.
3. Padroniza√ß√£o de colunas (`release_date` como `DATE`, `popularity` como `DOUBLE`, etc).
4. Extra√ß√£o autom√°tica de `ano_raw`, `mes_raw`, `dia_raw` a partir do caminho do arquivo S3.
5. Escrita do resultado em formato **Parquet**, **particionado por data**.

### üìù **Observa√ß√£o Importante**

> ‚ö†Ô∏è Os arquivos .json originais n√£o est√£o versionados neste reposit√≥rio para evitar o armazenamento de grandes volumes de dados.
> 
> 
> Eles s√£o obtidos automaticamente via API TMDB.
> 
> Caso deseje testar localmente, h√° amostras reduzidas dos dados em:
> 
> ```bash
> data/sample/json/
> 
> ```
> 

---

## üß© **Cria√ß√£o do Glue Catalog**

Para que as tabelas sejam consult√°veis via **AWS Athena**, √© necess√°rio criar e popular o cat√°logo de metadados no **AWS Glue**.

### üß© Passos do Processo

1. Criar um DataBase no Glue
2. Criar um Crawler
3. Consultar no Athena

---

## üßæ **Execu√ß√£o do Job no Glue**

### Par√¢metros esperados

O script recebe os seguintes par√¢metros no Glue Job:

| Par√¢metro | Descri√ß√£o |
| --- | --- |
| `S3_INPUT_PATH_MOVIES_CARTAZ` | Caminho S3 dos JSONs de filmes em cartaz |
| `S3_INPUT_PATH_MOVIES_POPULARES` | Caminho S3 dos JSONs de filmes populares |
| `S3_INPUT_PATH_MOVIES_GENEROS` | Caminho S3 dos JSONs de g√™neros |
| `S3_TARGET_PATH_MOVIES_CARTAZ` | Caminho de sa√≠da (Parquet) dos filmes em cartaz |
| `S3_TARGET_PATH_MOVIES_POPULARES` | Caminho de sa√≠da (Parquet) dos filmes populares |
| `S3_TARGET_PATH_MOVIES_GENEROS` | Caminho de sa√≠da (Parquet) dos g√™neros |
| `S3_INPUT_PATH_MOVIES` | Caminho S3 do csv de movies |
| `S3_INPUT_PATH_SERIES` | Caminho S3 do csv de series |
| `S3_TARGET_PATH_MOVIES` | Caminho de sa√≠da (Parquet) dos movies |
| `S3_TARGET_PATH_SERIES` | Caminho de sa√≠da (Parquet) das series |

---

## üßπ **Resultado Final**

‚úÖ Dados limpos e normalizados em **Parquet**

‚úÖ Particionados por data (`ano_raw`, `mes_raw`, `dia_raw`)

‚úÖ Catalogados no **Glue Data Catalog**

‚úÖ Consult√°veis via **Athena**

‚úÖ Prontos para consumo na **camada REFINED**