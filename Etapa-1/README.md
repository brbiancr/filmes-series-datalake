# Etapa 1 -  Camada RAW

> √â a **primeira camada do Data Lake**, respons√°vel por **armazenar os dados brutos**, do jeito que foram recebidos das fontes. Ela funciona como um **reposit√≥rio hist√≥rico e de rastreabilidade** ‚Äî se algo der errado em etapas posteriores, voc√™ pode sempre voltar aqui e reprocessar.
> 

## üîπ **Objetivo**

- **Preservar a integridade original** dos dados.
- Garantir que nada seja perdido ‚Äî nem erros, nem duplica√ß√µes, nem inconsist√™ncias.
- **Desacoplar a ingest√£o da transforma√ß√£o**, ou seja, deixar a coleta independente da limpeza.
- Servir como **base para reproduzir pipelines** (ex: se precisar reconstruir a camada tratada no futuro).

## üîπ Etapa 1A ‚Äì Ingest√£o  Batch

Nesta estapa deve ser contru√≠do um c√≥digo Python que ser√° executado dentro de um container Docker para carregar os dados locais dos arquivos disponibilizados para nuvem. Nesse caso utilizaremos, principalmente, a lib boto 3 como parte do processo de ingest√£o via batch para gera√ß√£o de arquivo (CSV).

![image 1.png](../assets/image%202.png)

### üß© Passos do Processo

1. Ler os arquivos **de filmes e s√©ries** no formato `.csv`, sem aplicar filtros ou transforma√ß√µes.
2. Fazer o upload desses arquivos diretamente para o bucket **`raw/`** no S3.

### üìù **Observa√ß√£o Importante**

> ‚ö†Ô∏è Os arquivos .csv originais n√£o est√£o versionados neste reposit√≥rio para evitar o armazenamento de grandes volumes de dados.
> 
> 
> Eles podem ser acessados diretamente no **bucket S3** configurado para este projeto.
> 
> Caso deseje testar localmente, h√° amostras reduzidas dos dados dispon√≠veis na pasta:
> 
> ```
> data/sample/
> ```
> 
> com apenas algumas linhas, para fins de demonstra√ß√£o.
> 

### üöÄ Executando o Pipeline via Docker

### Pr√©-requisitos

- Docker instalado
- CSVs de filmes e s√©ries dispon√≠veis na pasta `sample_data/` ou caminho correto
- Arquivo `.env` com as seguintes vari√°veis definidas:

```
# Caminhos dos arquivos CSV locais
CAMINHO_DO_ARQUIVO_LOCAL_MOVIES=../Filmes+e+series/movies.csv
CAMINHO_DO_ARQUIVO_LOCAL_SERIES=../Filmes+e+series/series.csv

# Bucket S3
NOME_DO_BUCKET_S3=meu-bucket
REGIAO_DO_BUCKET=us-east-1

# Perfil SSO (opcional)
NOME_DO_PERFIL_SSO=default

# Credenciais AWS
aws_access_key_id=SEU_ACCESS_KEY
aws_secret_access_key=SUA_SECRET_KEY
aws_session_token=SEU_SESSION_TOKEN  # opcional, se estiver usando STS ou SSO

```

### 1Ô∏è‚É£ Build da imagem

```bash
docker build -t csv_ingestion .
```

### 2Ô∏è‚É£ Executar o container

```bash
docker run --rm --env-file .env csv_ingestion
```

> O script far√° upload dos CSVs para o bucket S3 configurado.
> 
> 
> Logs de execu√ß√£o ser√£o exibidos no terminal.
> 

### 3Ô∏è‚É£ Rodando localmente (sem Docker)

```bash
python script.py
```

> O script ir√° ler os arquivos CSV definidos em CAMINHO_DO_ARQUIVO_LOCAL_MOVIES e CAMINHO_DO_ARQUIVO_LOCAL_SERIES e fazer o upload para o bucket S3 configurado.
> 
> 
> Logs de execu√ß√£o ser√£o exibidos no terminal.
> 

---

## üîπ Etapa 1B ‚Äì Ingest√£o de API

Nesta etapa iremos capturar dados do TMDB via `AWS Lambda` realizando requisi√ß√µes para a API. Os dados coletados devem ser persistidos em Amazon S3, camada RAW Zone, mantendo o formatto da origem (JSON). 

O objetivo desta etapa √© complementar os dados dos Filmes e S√©ries, carregados na Etapa 1B, com dados oriundos do TMDB.

![image.png](../assets/image%201.png)

### üß© Passos do Processo

1. Criar uma **imagem Docker** baseada no **Amazon Linux 2023** e instalar `pip` e `zip`.
2. Instalar as bibliotecas `requests` e `python-dotenv`, compact√°-las em `minha-camada.zip` e subir como **layer no AWS Lambda**.
3. Criar a fun√ß√£o Lambda para **consumir a API do TMDB** usando as credenciais configuradas no `.env`.
4. **Gerar arquivos JSON** com os dados de filmes e s√©ries.
5. Fazer **upload autom√°tico** dos arquivos para o **bucket S3**, na **camada RAW** do Data Lake.

### üìù **Observa√ß√£o Importante**

> ‚ö†Ô∏è Os arquivos .json originais n√£o est√£o versionados neste reposit√≥rio para evitar o armazenamento de grandes volumes de dados.
> 
> 
> Eles podem ser acessados diretamente no **bucket S3** configurado para este projeto.
> 
> Caso deseje testar localmente, h√° amostras reduzidas dos dados dispon√≠veis na pasta:
> 
> ```
> data/sample/
> ```
> 
> com apenas algumas linhas, para fins de demonstra√ß√£o.
> 

### 1Ô∏è‚É£ Cria√ß√£o da Layer (Camada)

Para criar a layer do Lambda, ser√° utilizada uma imagem m√≠nima do **Amazon Linux 2023**, garantindo compatibilidade com o ambiente do AWS Lambda.

```docker
FROM amazonlinux:2023

RUN yum update -y
RUN yum install -y \
    python3-pip \
    zip

RUN yum -y clean all

```

### 2Ô∏è‚É£ Constru√ß√£o da Imagem Docker

```bash
docker build -t amazonlinuxpython39 .
```

### 3Ô∏è‚É£  Acessando o Container

Inicie o container em modo interativo:

```bash
docker run -it amazonlinuxpython39 bash
```

### 4Ô∏è‚É£ Estrutura de Pastas no Container

Crie as pastas que ir√£o armazenar as depend√™ncias de layer:

```bash
cd ~
mkdir layer_dir
cd layer_dir/
mkdir python
cd python/
pwd
```

### 5Ô∏è‚É£ Instala√ß√£o das Depend√™ncias

Dentro da pasta python, instale as bibliotecas necess√°rias:

```bash
pip3 install requests -t .
pip3 install python-dotenv -t .
```

### 6Ô∏è‚É£ Compacta√ß√£o da Layer

Ap√≥s instalar as depend√™ncias, compacte o conte√∫do:

```bash
zip -r minha-camada.zip .
```

### 7Ô∏è‚É£ Copiando a Layer para a M√°quina Local

Abra outro terminal e copie o .zip gerado do container para sua m√°quina local.

Substitua CONTAINER_ID pelo ID do seu container:

```bash
docker cp CONTAINER_ID:/root/layer_dir/minha-camada.zip ./
```

### 8Ô∏è‚É£ Incluindo o Script de Upload no Container

Em outro terminal:

```bash
docker exec -it CONTAINER_ID mkdir -p /root/layer_dir
docker cp upload_s3.py CONTAINER_ID:/root/layer_dir/
```

Depois, dentro do container, compacte novamente:

```bash
cd /root/layer_dir
zip -r minha-camada.zip .
```

E copie novamente para a m√°quina local: 

```bash
docker cp CONTAINER_ID:/root/layer_dir/minha-camada.zip ./
```

### 9Ô∏è‚É£ Upload da Layer no AWS Lambda

- Acesse o **AWS Management Console**
- V√° at√© **Lambda ‚Üí Layers ‚Üí Create layer**
- Fa√ßa upload do arquivo `minha-camada.zip`
- Selecione a vers√£o do Python compat√≠vel (ex: **Python 3.9**)
- Clique em **Create**

### 1Ô∏è‚É£0Ô∏è‚É£ Cria√ß√£o da Fun√ß√£o Lambd

1. No console do **AWS Lambda**, crie uma nova fun√ß√£o 
2. Anexe a layer criada anteriormente
3. Fa√ßa o upload do seu script `upload_s3.py`
4. Configure as **vari√°veis de ambiente** listadas no `.env`:

| Vari√°vel | Descri√ß√£o |
| --- | --- |
| `NOME_DO_BUCKET_S3` | Nome do bucket S3 de destino |
| `REGIAO_DO_BUCKET` | Regi√£o do bucket |
| `NOME_DO_PERFIL_SSO` | Nome do perfil de autentica√ß√£o |
| `aws_access_key_id` | Chave de acesso AWS |
| `aws_secret_access_key` | Chave secreta AWS |
| `aws_session_token` | Token de sess√£o tempor√°rio |

### ‚úÖ Resultado Esperado

Ao executar a fun√ß√£o Lambda, os dados da API do TMDB ser√£o baixados e armazenados automaticamente no bucket S3, compondo a **camada RAW** do Data Lake.